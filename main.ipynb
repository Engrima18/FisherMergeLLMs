{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqMwvfdIGgHe",
        "outputId": "c265fc23-ff36-417f-ffdb-ee282a692b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content\n",
            "Downloading news-headlines-dataset-for-sarcasm-detection.zip to /content\n",
            "  0% 0.00/3.30M [00:00<?, ?B/s]\n",
            "100% 3.30M/3.30M [00:00<00:00, 38.4MB/s]\n",
            "mkdir: cannot create directory ‘dataset’: File exists\n"
          ]
        }
      ],
      "source": [
        "dataset_author = \"rmisra\"\n",
        "dataset_name = \"news-headlines-dataset-for-sarcasm-detection\"\n",
        "dataset_path = dataset_author + \"/\" + dataset_name\n",
        "\n",
        "# Mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "kaggle_creds_path = \"/content/drive/MyDrive/my_kaggle/kaggle.json\"\n",
        "\n",
        "# remove kaggle directory if already exists, then crate it\n",
        "!pwd\n",
        "!rm -r ~/.kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp {kaggle_creds_path} ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! pip install kaggle --quiet\n",
        "! kaggle datasets download -d {dataset_path}\n",
        "\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(f'{dataset_name}.zip', 'r')\n",
        "\n",
        "! mkdir dataset\n",
        "zip_ref.extractall('/content/dataset')\n",
        "zip_ref.close()\n",
        "! rm {dataset_name}.zip\n",
        "\n",
        "# Unmount your Google Drive\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "kBkXeBCZHrk0",
        "outputId": "274900db-d858-4028-cc22-3672694a36da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28614</th>\n",
              "      <td>1</td>\n",
              "      <td>jews to celebrate rosh hashasha or something</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28615</th>\n",
              "      <td>1</td>\n",
              "      <td>internal affairs investigator disappointed con...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28616</th>\n",
              "      <td>0</td>\n",
              "      <td>the most beautiful acceptance speech this week...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28617</th>\n",
              "      <td>1</td>\n",
              "      <td>mars probe destroyed by orbiting spielberg-gat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28618</th>\n",
              "      <td>1</td>\n",
              "      <td>dad clarifies this not a food stop</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28619 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       is_sarcastic                                           headline\n",
              "0                 1  thirtysomething scientists unveil doomsday clo...\n",
              "1                 0  dem rep. totally nails why congress is falling...\n",
              "2                 0  eat your veggies: 9 deliciously different recipes\n",
              "3                 1  inclement weather prevents liar from getting t...\n",
              "4                 1  mother comes pretty close to using word 'strea...\n",
              "...             ...                                                ...\n",
              "28614             1       jews to celebrate rosh hashasha or something\n",
              "28615             1  internal affairs investigator disappointed con...\n",
              "28616             0  the most beautiful acceptance speech this week...\n",
              "28617             1  mars probe destroyed by orbiting spielberg-gat...\n",
              "28618             1                 dad clarifies this not a food stop\n",
              "\n",
              "[28619 rows x 2 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "try:\n",
        "    df = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines = True)\n",
        "except:\n",
        "    df = pd.read_json(\"/content/dataset/Sarcasm_Headlines_Dataset_v2.json\", lines = True)\n",
        "df.head()\n",
        "df.drop(columns=\"article_link\", axis=1, inplace=True)\n",
        "df.head(5)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t5A1WQawVnMj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QjD_rGI-e8b5"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_encode(texts, tokenizer):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "def df_to_dataset(dataframe, tokenizer, shuffle=True, batch_size=32):\n",
        "    dataframe = dataframe.copy()\n",
        "    labels = dataframe.pop('is_sarcastic')\n",
        "    encoded_texts = tokenize_and_encode(dataframe['headline'].tolist(), tokenizer)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((encoded_texts['input_ids'], labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YZpksQHB09Pf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No TPU available!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    BATCH_SIZE = 32 * tpu_strategy.num_replicas_in_sync\n",
        "except:\n",
        "    print(\"No TPU available!\")\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CBmXVzCvBnnZ"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "train_data = df_to_dataset(train_df, tokenizer, shuffle=True, batch_size=BATCH_SIZE)\n",
        "test_data = df_to_dataset(test_df, tokenizer, shuffle=False, batch_size=BATCH_SIZE)\n",
        "train_data = train_data.prefetch(buffer_size=AUTOTUNE)\n",
        "test_data = test_data.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z-YStsUTM1q"
      },
      "source": [
        "# Import fine-tuned RoBERTa versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gKa-3RUHTSKr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "mnli_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "            \"textattack/roberta-base-MNLI\", from_pt=True\n",
        "        )\n",
        "\n",
        "sst2_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "            \"textattack/roberta-base-SST-2\", from_pt=True\n",
        "        )\n",
        "\n",
        "### add RoBERTa for sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLzm1-6f3LXH"
      },
      "source": [
        "# Fine-tune Base RoBERTa\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KjP3Zy9O1ZUr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No TPU available!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    with tpu_strategy.scope():\n",
        "        model_name = \"roberta-base\"\n",
        "        model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True, num_labels=2)\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.),\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=[tf.metrics.SparseCategoricalAccuracy()],\n",
        "            )\n",
        "except:\n",
        "    print(\"No TPU available!\")\n",
        "    model_name = \"roberta-base\"\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True, num_labels=2)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[tf.metrics.SparseCategoricalAccuracy()],\n",
        "        )\n",
        "\n",
        "try:\n",
        "    model.load_weights(f'.\\\\models\\\\{model_name.replace(\"-\",\"_\")}_ft.h5')\n",
        "except:\n",
        "    history=model.fit(train_data, validation_data=test_data, epochs=6, verbose=1)\n",
        "    model.save_weights(f'.\\\\models\\\\{model_name.replace(\"-\",\"_\")}_ft1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DwPv3MYECYM",
        "outputId": "ff998296-05e7-433f-9ea5-7c04c1d241b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_roberta_for_sequence_classification_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
            "_________________________________________________________________\n",
            "classifier (TFRobertaClassif multiple                  592130    \n",
            "=================================================================\n",
            "Total params: 124,647,170\n",
            "Trainable params: 124,647,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.load_weights(f'.\\\\models\\\\{model_name.replace(\"-\",\"_\")}_ft.h5')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GCmvEOl2g00"
      },
      "source": [
        "# Take the weights from my models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "83AQ3yRF-QA6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def fetch_mergeable_vars(mod, to_np=False):\n",
        "    \"\"\"Fetches trainable variables of a given model and converts them to NumPy arrays.\n",
        "\n",
        "    This function supports both models running under a TensorFlow distribution strategy (e.g., TPUs)\n",
        "    and standard models not using distribution strategies (e.g., models downloaded from Hugging Face).\n",
        "\n",
        "    Args:\n",
        "        model: TensorFlow or Hugging Face model whose trainable variables are to be fetched.\n",
        "        strategy: (Optional) The TensorFlow distribution strategy under which the model is running.\n",
        "                  If None, the model is assumed not to be under a distribution strategy.\n",
        "\n",
        "    Returns:\n",
        "        A list of NumPy arrays corresponding to the model's trainable variables.\n",
        "    \"\"\"\n",
        "    def fetch_variables():\n",
        "        body, *head = mod.layers\n",
        "        return [v.numpy() for v in body.trainable_variables]\n",
        "\n",
        "    def distributed_fetch():\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "        body, *head = mod.layers\n",
        "        return strategy.run(lambda: body.trainable_variables)\n",
        "\n",
        "    if to_np:\n",
        "\n",
        "        try:\n",
        "            per_replica_variables = distributed_fetch()\n",
        "            numpy_variables = []\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "            for v in strategy.experimental_local_results(per_replica_variables):\n",
        "                numpy_variables.extend([var.numpy() for var in v])\n",
        "        except:\n",
        "            numpy_variables = fetch_variables()\n",
        "\n",
        "        vars = np.array(numpy_variables, dtype=object)\n",
        "\n",
        "    else:\n",
        "        body, *head = mod.layers\n",
        "        vars = body.trainable_variables\n",
        "\n",
        "    return vars\n",
        "\n",
        "def clone_model(model):\n",
        "    cloned = model.__class__(model.config)\n",
        "    cloned(model.dummy_inputs)\n",
        "    cloned.set_weights(model.get_weights())\n",
        "    return cloned\n",
        "\n",
        "\n",
        "# def assign_params(models_list):\n",
        "#     \"\"\"Assigns values from a list of NumPy arrays to the model's trainable variables.\n",
        "\n",
        "#     This function supports both models running under a TensorFlow distribution strategy (e.g., TPUs)\n",
        "#     and standard models not using distribution strategies (e.g., models downloaded from Hugging Face).\n",
        "\n",
        "#     Args:\n",
        "#         model: TensorFlow or Hugging Face model whose trainable variables are to be updated.\n",
        "#         numpy_variables: A list of NumPy arrays containing the new values for the model's trainable variables.\n",
        "#         strategy: (Optional) The TensorFlow distribution strategy under which the model is running.\n",
        "#                   If None, the model is assumed not to be under a distribution strategy.\n",
        "#     \"\"\"\n",
        "#     # just clone the first model in the list to initialize the parameters\n",
        "#     new_model = clone_model(models_list[0])\n",
        "\n",
        "#     vars_arr = np.array([fetch_mergeable_vars(mod) for mod in models_list], dtype=object)\n",
        "#     # then we have to multiply each matrix in this vars_arr by a corresponding fisher matrix (different for each matrix for each model)\n",
        "#     # the first goal: create a fishers_arr of the same dim of vars_arr (3, 197) with\n",
        "\n",
        "#     def assign_variables():\n",
        "#         body, *head = new_model.layers\n",
        "#         for var, new_val in zip(body.trainable_variables, numpy_variables):\n",
        "#             var.assign(new_val)\n",
        "\n",
        "#     @tf.function\n",
        "#     def distributed_assign():\n",
        "#         strategy = tf.distribute.get_strategy()\n",
        "#         body, *head = model.layers\n",
        "#         for var, new_val in zip(body.trainable_variables, numpy_variables):\n",
        "#             strategy.run(lambda: var.assign(new_val))\n",
        "\n",
        "#     try:\n",
        "#         distributed_assign()\n",
        "#     except:\n",
        "#         assign_variables()\n",
        "\n",
        "\n",
        "\n",
        "# numpy_variables = fetch_trainable_vars(model)\n",
        "numpy_variables_distributed = fetch_mergeable_vars(model)\n",
        "numpy_variables = fetch_mergeable_vars(mnli_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "73UiIhC_FYRz"
      },
      "outputs": [],
      "source": [
        "def _fisher_for_batch(batch, model, variables):\n",
        "    num_labels = model.num_labels\n",
        "    batch = tf.expand_dims(batch, axis=0)\n",
        "    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "        tape.watch(variables)\n",
        "        logits = model(batch, training=False).logits\n",
        "        logits = tf.squeeze(logits)\n",
        "        \n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "        print(batch.shape)\n",
        "        print(probs.shape)\n",
        "        #print(batch.shape)\n",
        "        \n",
        "        res = []\n",
        "\n",
        "        for i, single_log in enumerate(log_probs):\n",
        "            with tape.stop_recording():\n",
        "                tmp = [probs[i]*tf.square(grad) for grad in tape.gradient(single_log, variables)]\n",
        "                res.append(tmp)\n",
        "                # sq_grads = tf.square(tape.gradient(log_probs, variables))\n",
        "                # sq_grads = [tf.square(tape.gradient(log_probs[i], variables)) for i in range(num_labels)] ### REF\n",
        "            #tmp=[probs[:,i]*sq_grads[i] for i in range(num_labels)]\n",
        "            # tmp = tf.math.multply(probs, sr_grad)\n",
        "            # logits = tf.squeeze(logits, axis=0)\n",
        "            # log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "            # probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "            # grads = tape.gradient(log_probs, variables)\n",
        "            # sq_grads = []\n",
        "            # for i in range(num_labels):\n",
        "            #     log_prob = log_probs[:, i]\n",
        "            #     grad = tape.gradient(log_prob, variables)\n",
        "            #     sq_grad = [probs[:, i] * tf.square(g) for g in grad]\n",
        "            #     sq_grads.append(sq_grad)\n",
        "\n",
        "            # example_fisher = [tf.reduce_sum(g, axis=0) for g in zip(*sq_grads)]\n",
        "\n",
        "    return probs, log_probs, res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7-shjII7H4cP"
      },
      "outputs": [],
      "source": [
        "def preprocess(input, label):\n",
        "    return input\n",
        "\n",
        "tr = train_data.map(preprocess)\n",
        "mergeable_vars = fetch_mergeable_vars(model, to_np=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([768, 768])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mergeable_vars[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "197"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(mergeable_vars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 195)\n"
          ]
        }
      ],
      "source": [
        "for example in tr:\n",
        "   print(example.shape)\n",
        "   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(195,)\n"
          ]
        }
      ],
      "source": [
        "for example in tr.unbatch():\n",
        "   print(example.shape)\n",
        "   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "L138VWpFHtcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 195)\n",
            "(2,)\n"
          ]
        },
        {
          "ename": "ResourceExhaustedError",
          "evalue": "failed to allocate memory [Op:Mul]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[79], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m tr\u001b[38;5;241m.\u001b[39munbatch():\n\u001b[1;32m----> 2\u001b[0m    probs, log_probs, res \u001b[38;5;241m=\u001b[39m \u001b[43m_fisher_for_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmergeable_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m    \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[78], line 20\u001b[0m, in \u001b[0;36m_fisher_for_batch\u001b[1;34m(batch, model, variables)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, single_log \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(log_probs):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tape\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m---> 20\u001b[0m         tmp \u001b[38;5;241m=\u001b[39m [probs[i]\u001b[38;5;241m*\u001b[39mtf\u001b[38;5;241m.\u001b[39msquare(grad) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m tape\u001b[38;5;241m.\u001b[39mgradient(single_log, variables)]\n\u001b[0;32m     21\u001b[0m         res\u001b[38;5;241m.\u001b[39mappend(tmp)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m# sq_grads = tf.square(tape.gradient(log_probs, variables))\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# sq_grads = [tf.square(tape.gradient(log_probs[i], variables)) for i in range(num_labels)] ### REF\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#tmp=[probs[:,i]*sq_grads[i] for i in range(num_labels)]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# example_fisher = [tf.reduce_sum(g, axis=0) for g in zip(*sq_grads)]\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[78], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, single_log \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(log_probs):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tape\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m---> 20\u001b[0m         tmp \u001b[38;5;241m=\u001b[39m [\u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m tape\u001b[38;5;241m.\u001b[39mgradient(single_log, variables)]\n\u001b[0;32m     21\u001b[0m         res\u001b[38;5;241m.\u001b[39mappend(tmp)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m# sq_grads = tf.square(tape.gradient(log_probs, variables))\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# sq_grads = [tf.square(tape.gradient(log_probs[i], variables)) for i in range(num_labels)] ### REF\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#tmp=[probs[:,i]*sq_grads[i] for i in range(num_labels)]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# example_fisher = [tf.reduce_sum(g, axis=0) for g in zip(*sq_grads)]\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1367\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1363\u001b[0m   \u001b[38;5;66;03m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1365\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(x, y, force_same_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1367\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1369\u001b[0m   \u001b[38;5;66;03m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[0;32m   1370\u001b[0m   \u001b[38;5;66;03m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1373\u001b[0m   \u001b[38;5;66;03m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m   \u001b[38;5;66;03m# informative.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__r\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m op_name):\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1710\u001b[0m, in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1708\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m sparse_tensor\u001b[38;5;241m.\u001b[39mSparseTensor(y\u001b[38;5;241m.\u001b[39mindices, new_vals, y\u001b[38;5;241m.\u001b[39mdense_shape)\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1710\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:530\u001b[0m, in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.multiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultiply\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    485\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns an element-wise x * y.\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m  For example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m   * InvalidArgumentError: When `x` and `y` have incompatible shapes or types.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6235\u001b[0m, in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   6234\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 6235\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   6237\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6941\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6939\u001b[0m message \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6940\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 6941\u001b[0m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_status_to_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
            "\u001b[1;31mResourceExhaustedError\u001b[0m: failed to allocate memory [Op:Mul]"
          ]
        }
      ],
      "source": [
        "for example in tr.unbatch():\n",
        "   probs, log_probs, res = _fisher_for_batch(example, model, mergeable_vars)\n",
        "   break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, watch_accessed_variables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# print(batch.shape)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     tape\u001b[38;5;241m.\u001b[39mwatch(variables)\n\u001b[1;32m---> 17\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# logits = tf.squeeze(logits)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mlog_softmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\keras\\engine\\base_layer.py:1037\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1040\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\transformers\\modeling_tf_utils.py:420\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    419\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:1374\u001b[0m, in \u001b[0;36mTFRobertaForSequenceClassification.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(ROBERTA_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1367\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFSequenceClassifierOutput, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;124;03m    labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1374\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1386\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1387\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output, training\u001b[38;5;241m=\u001b[39mtraining)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\keras\\engine\\base_layer.py:1037\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1040\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\transformers\\modeling_tf_utils.py:420\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    419\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:703\u001b[0m, in \u001b[0;36mTFRobertaMainLayer.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    700\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m extended_attention_mask[:, :, \u001b[38;5;241m-\u001b[39mseq_length:, :]\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    702\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m--> 703\u001b[0m         attention_mask, (attention_mask_shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[43mattention_mask_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    704\u001b[0m     )\n\u001b[0;32m    706\u001b[0m \u001b[38;5;66;03m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;66;03m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;66;03m# positions we want to attend and -10000.0 for masked positions.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;66;03m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(extended_attention_mask, dtype\u001b[38;5;241m=\u001b[39membedding_output\u001b[38;5;241m.\u001b[39mdtype)\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "def preprocess(input, label):\n",
        "    return input\n",
        "\n",
        "tr = train_data.map(preprocess)\n",
        "variables = fetch_mergeable_vars(model, to_np=False)\n",
        "\n",
        "for batch in train_data:\n",
        "    break\n",
        "\n",
        "# the function\n",
        "num_labels = model.num_labels\n",
        "# example = tf.expand_dims(example, axis=0)\n",
        "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "\n",
        "    # print(batch.shape)\n",
        "    tape.watch(variables)\n",
        "    logits = model(batch, training=False).logits\n",
        "    # logits = tf.squeeze(logits)\n",
        "    \n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    print(log_probs.shape)\n",
        "    probs = tf.nn.softmax(logits, axis=-1)\n",
        "    # tape.gradient(log_probs, variables)\n",
        "    # res = []\n",
        "\n",
        "    # for i, single_log in enumerate(log_probs):\n",
        "    #     with tape.stop_recording():\n",
        "    #         prova = tape.gradient(single_log, variables)\n",
        "            # prova = np.array(tape.gradient(single_log, variables), dtype=object)\n",
        "            # tmp = tf.Constant([probs[i]*tf.square(tf.Constant(grad)) for grad in tape.gradient(single_log, variables)])\n",
        "            # res.append(tmp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<MapDataset shapes: (None, 195), types: tf.int32>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "for example in tr.unbatch():\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(195,), dtype=int32, numpy=\n",
              "array([    0, 11613,  1627,  3827,  1797,   960,    11, 22791,     2,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1])>"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([    0,   397,  7789,   628,  1654,     7,   304, 10525,    37,\n",
              "          21,  6549,    13,  1618,   419,     2,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1])"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(example.as_numpy_iterator()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[75], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m probs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tape\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# sq_grads = tf.square(tape.gradient(log_probs, variables))\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     sq_grads \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39msquare(tape\u001b[38;5;241m.\u001b[39mgradient(log_probs[:,i], mergeable_vars)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_labels)]\n",
            "Cell \u001b[1;32mIn[75], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m probs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tape\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# sq_grads = tf.square(tape.gradient(log_probs, variables))\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     sq_grads \u001b[38;5;241m=\u001b[39m [\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmergeable_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_labels)]\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:10424\u001b[0m, in \u001b[0;36msquare\u001b[1;34m(x, name)\u001b[0m\n\u001b[0;32m  10422\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m  10423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 10424\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msquare_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10425\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10426\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m  10427\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:10459\u001b[0m, in \u001b[0;36msquare_eager_fallback\u001b[1;34m(x, name, ctx)\u001b[0m\n\u001b[0;32m  10458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquare_eager_fallback\u001b[39m(x, name, ctx):\n\u001b[1;32m> 10459\u001b[0m   _attr_T, (x,) \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex128\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10460\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [x]\n\u001b[0;32m  10461\u001b[0m   _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:265\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[1;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[1;32m--> 265\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[0;32m    267\u001b[0m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[0;32m    268\u001b[0m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1566\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1561\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1563\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[0;32m   1565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1566\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1569\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:346\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    344\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    345\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m--> 346\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:283\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    282\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 283\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[0;32m    286\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:308\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    307\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    105\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
          ]
        }
      ],
      "source": [
        "num_labels = model.num_labels\n",
        "example = tf.expand_dims(example, axis=0)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    tape.watch(mergeable_vars)\n",
        "    logits = model(example, training=False).logits\n",
        "\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    with tape.stop_recording():\n",
        "        # sq_grads = tf.square(tape.gradient(log_probs, variables))\n",
        "        sq_grads = [tf.square(tape.gradient(log_probs[:,i], mergeable_vars)) for i in range(num_labels)]\n",
        "    #tmp=[probs[:,i]*sq_grads[i] for i in range(num_labels)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tape.gradient(log_probs[:,1], mergeable_vars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYWO32L5-mvi"
      },
      "outputs": [],
      "source": [
        "sq2 = np.array(sq, dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBM5CqL8YVQt"
      },
      "outputs": [],
      "source": [
        "probs2 = probs.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4Ej8jnLc1cs",
        "outputId": "ed5b22cf-b2f0-4649-c9a0-acd54fa6ce51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(256, 2)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probs2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-HwCA3OZTT_",
        "outputId": "36e7c967-28f9-451d-c936-fc6fbe584f88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(197,)"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sq2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "C5P3dfl5YzLJ",
        "outputId": "8aaf6be4-26f5-48de-b3df-8c349d19f3b3"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (256,2) (197,) ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-56b49dfd644e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msq2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (256,2) (197,) "
          ]
        }
      ],
      "source": [
        "np.multiply(probs2, sq2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1sh5grkEacm",
        "outputId": "d06ecc1b-74fa-4a01-e613-71ebfc8d0325"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "197"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(numpy_variables_distributed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiOupcT26iXE"
      },
      "outputs": [],
      "source": [
        "prova = np.array(numpy_variables, dtype=object)\n",
        "prova2 = np.array(numpy_variables_distributed, dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApYC44-NNrX4"
      },
      "outputs": [],
      "source": [
        "models_list = [model, mnli_model, sst2_model]\n",
        "vars_list = np.array([fetch_trainable_vars(mod) for mod in models_list], dtype=object)\n",
        "pp=np.sum(vars_list, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZkIlgEPOfBm",
        "outputId": "33b77312-63e6-4b37-b7e5-b8b84a21868e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 197)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vars_list.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeN8UmqXPxsj",
        "outputId": "39656d66-80f7-470b-8cc4-b42c9d22f637"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(197,)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pp=np.sum(vars_list, axis=0)\n",
        "pp.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw51sJ5sRxJt"
      },
      "source": [
        "# Fisher Averaging implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYxhqmmg3DfM"
      },
      "source": [
        "### hf_util.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRbSC9Rf3CL4"
      },
      "outputs": [],
      "source": [
        "def get_body_and_head(\n",
        "    model: Union[TFBertPreTrainedModel, TFRobertaPreTrainedModel]\n",
        ") -> Tuple[tf.keras.layers.Layer, tf.keras.layers.Layer]:\n",
        "    body, *head = model.layers\n",
        "    if not head:\n",
        "        head = None\n",
        "    elif len(head) > 1:\n",
        "        raise ValueError(\n",
        "            f\"Expected model to have a single 'head' layer. Instead found {len(head)}. TODO: Support this.\"\n",
        "        )\n",
        "    else:\n",
        "        head = head[0]\n",
        "    return body, head\n",
        "\n",
        "\n",
        "def get_body(model):\n",
        "    return get_body_and_head(model)[0]\n",
        "\n",
        "\n",
        "def get_mergeable_variables(model):\n",
        "    return get_body_and_head(model)[0].trainable_variables\n",
        "\n",
        "\n",
        "def clone_model(model):\n",
        "    cloned = model.__class__(model.config)\n",
        "    cloned(model.dummy_inputs)\n",
        "    cloned.set_weights(model.get_weights())\n",
        "    return cloned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-AJ7yxXWHVf"
      },
      "source": [
        "### evaluation.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVz0oVmpWKms"
      },
      "outputs": [],
      "source": [
        "# import datasets as hfds\n",
        "\n",
        "# def load_metric_for_glue_task(task: str):\n",
        "#     return hfds.load_metric(\"glue\", task)\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataset: tf.data.Dataset, metric: hfds.Metric):\n",
        "    for model_input, gold_references in dataset:\n",
        "        model_predictions = model(model_input).logits\n",
        "        model_predictions = tf.argmax(model_predictions, axis=-1)\n",
        "        metric.add_batch(predictions=model_predictions, references=gold_references)\n",
        "    return metric.compute()\n",
        "\n",
        "\n",
        "def average_score(score):\n",
        "    return sum(score.values()) / len(score.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcVz0L_kVxvu"
      },
      "source": [
        "### fisher.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUpscA3yV30a"
      },
      "outputs": [],
      "source": [
        "def _batch_size(batch):\n",
        "    return tf.shape(batch[\"input_ids\"])[0]\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def _compute_exact_fisher_for_batch(batch, model, variables, expectation_wrt_logits):\n",
        "    assert expectation_wrt_logits, \"TODO: Handle sampling from logits.\"\n",
        "    num_labels = model.num_labels\n",
        "\n",
        "    @tf.function\n",
        "    def fisher_single_example(single_example_batch):\n",
        "        \"\"\"\n",
        "        NOTE: I wrote this with Hugging Face classifiers in mind. There is\n",
        "        probably a good way to do the same thing but with more customizability\n",
        "        to support alternate forms of models.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "            tape.watch(variables)\n",
        "\n",
        "            logits = model(single_example_batch, training=False).logits\n",
        "            # The batch dimension must be 1 to call the model, so we remove it\n",
        "            # here.\n",
        "            logits = tf.squeeze(logits, axis=0)\n",
        "\n",
        "            log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "            probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "            sq_grads = []\n",
        "            for i in range(num_labels):\n",
        "                log_prob = log_probs[i]\n",
        "                with tape.stop_recording():\n",
        "                    grad = tape.gradient(log_prob, variables)\n",
        "                    sq_grad = [probs[i] * tf.square(g) for g in grad]\n",
        "                    sq_grads.append(sq_grad)\n",
        "            # Take the average across logits. The per-logit weight was added\n",
        "            # earlier as each per-logit square gradient was weighted by the\n",
        "            # probability of the class according to the output distribution.\n",
        "            example_fisher = [tf.reduce_sum(g, axis=0) for g in zip(*sq_grads)]\n",
        "\n",
        "        return example_fisher\n",
        "\n",
        "    batch = {k: tf.expand_dims(v, axis=1) for k, v in batch.items()}\n",
        "\n",
        "    fishers = tf.vectorized_map(fisher_single_example, batch)\n",
        "    return [tf.reduce_sum(f, axis=0) for f in fishers]\n",
        "\n",
        "\n",
        "def compute_fisher_for_model(\n",
        "    model, dataset: tf.data.Dataset, expectation_wrt_logits=True\n",
        "):\n",
        "    variables = hf_util.get_mergeable_variables(model)\n",
        "\n",
        "    fishers = [\n",
        "        tf.Variable(tf.zeros(w.shape), trainable=False, name=f\"fisher/{w.name}\")\n",
        "        for w in variables\n",
        "    ]\n",
        "\n",
        "    n_examples = 0\n",
        "    for batch, _ in dataset:\n",
        "        n_examples += _batch_size(batch)\n",
        "        batch_fishers = _compute_exact_fisher_for_batch(\n",
        "            batch, model, variables, expectation_wrt_logits=expectation_wrt_logits\n",
        "        )\n",
        "        for f, bf in zip(fishers, batch_fishers):\n",
        "            f.assign_add(bf)\n",
        "\n",
        "    for fisher in fishers:\n",
        "        fisher.assign(fisher / float(n_examples))\n",
        "\n",
        "    return fishers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAgTaE_fWcrZ"
      },
      "source": [
        "### merging.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNaFK8KrWe7F"
      },
      "outputs": [],
      "source": [
        "\"\"\"The code for actually performing the merge.\"\"\"\n",
        "import collections\n",
        "from typing import Optional, Sequence\n",
        "import datasets as hfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from model_merging import hf_util\n",
        "from model_merging import evaluation\n",
        "\n",
        "MergeResult = collections.namedtuple(\"MergeResult\", [\"coefficients\", \"score\"])\n",
        "\n",
        "\n",
        "def print_merge_result(result: MergeResult):\n",
        "    print(f\"Merging coefficients: {result.coefficients}\")\n",
        "    print(\"Scores:\")\n",
        "    for name, value in result.score.items():\n",
        "        print(f\"  {name}: {value}\")\n",
        "\n",
        "\n",
        "def create_pairwise_grid_coeffs(n_weightings: int):\n",
        "    n_weightings -= 2\n",
        "    denom = n_weightings + 1\n",
        "    weightings = [((i + 1) / denom, 1 - (i + 1) / denom) for i in range(n_weightings)]\n",
        "    weightings = [(0.0, 1.0)] + weightings + [(1.0, 0.0)]\n",
        "    weightings.reverse()\n",
        "    return weightings\n",
        "\n",
        "\n",
        "def create_random_coeffs(n_models: int, n_weightings: int, seed: Optional[int] = None):\n",
        "    if seed is not None:\n",
        "        tf.random.set_seed(seed)\n",
        "    dist = tfp.distributions.Dirichlet(tf.ones([n_models]))\n",
        "    return dist.sample(n_weightings, seed=seed).numpy().tolist()\n",
        "\n",
        "\n",
        "def _merge_with_coeffs(\n",
        "    output_variables: Sequence[tf.Variable],\n",
        "    variables_to_merge: Sequence[Sequence[tf.Variable]],\n",
        "    coefficients: Sequence[float],\n",
        "    fishers=None,\n",
        "    fisher_floor: float = 1e-6,\n",
        "    favor_target_model=True,\n",
        "    normalization_constants=None,\n",
        "):\n",
        "    n_models = len(variables_to_merge)\n",
        "    assert len(coefficients) == n_models\n",
        "\n",
        "    if fishers is None:\n",
        "        fishers = n_models * [1.0]\n",
        "    else:\n",
        "        assert len(fishers) == n_models\n",
        "\n",
        "    if normalization_constants is not None:\n",
        "        assert len(normalization_constants) == n_models\n",
        "        coefficients = [w / n for w, n in zip(coefficients, normalization_constants)]\n",
        "\n",
        "    for i, var in enumerate(output_variables):\n",
        "        lhs, rhs = [], []\n",
        "        for j, (mvars, coeff, fisher) in enumerate(\n",
        "            zip(variables_to_merge, coefficients, fishers)\n",
        "        ):\n",
        "            diag = fisher if isinstance(fisher, float) else fisher[i]\n",
        "            if not favor_target_model or j == 0:\n",
        "                diag = tf.maximum(diag, fisher_floor)\n",
        "            mvar = mvars[i]\n",
        "            tmp = coeff * diag\n",
        "            lhs.append(tmp)\n",
        "            rhs.append(tmp * mvar)\n",
        "        rhs = tf.reduce_sum(rhs, axis=0)\n",
        "        lhs = tf.reduce_sum(lhs, axis=0)\n",
        "        var.assign(rhs / lhs)\n",
        "\n",
        "\n",
        "def _l2_norm_of_fisher(fisher):\n",
        "    norm_const = tf.reduce_sum([tf.reduce_sum(tf.square(d)) for d in fisher])\n",
        "    return tf.sqrt(norm_const)\n",
        "\n",
        "\n",
        "def generate_merged_for_coeffs_set(\n",
        "    mergeable_models,\n",
        "    coefficients_set: Sequence[Sequence[float]],\n",
        "    fishers=None,\n",
        "    fisher_floor: float = 1e-6,\n",
        "    favor_target_model=True,\n",
        "    normalize_fishers=True,\n",
        "):\n",
        "    # Create the model to yield, then handle the norm_constants\n",
        "    if normalize_fishers and fishers is not None:\n",
        "        norm_constants = [_l2_norm_of_fisher(f) for f in fishers]\n",
        "    else:\n",
        "        norm_constants = None\n",
        "\n",
        "    # The first model in the list of mergeable models is the \"target\" model and\n",
        "    # the rest are \"donor\" models.\n",
        "    output_model = hf_util.clone_model(mergeable_models[0])\n",
        "    output_variables = hf_util.get_mergeable_variables(output_model)\n",
        "\n",
        "    variables_to_merge = [hf_util.get_mergeable_variables(m) for m in mergeable_models]\n",
        "\n",
        "    # Make sure that all of the variable lists contain exactly the same number\n",
        "    # of variables.\n",
        "    assert len({len(output_variables)} | set(len(v) for v in variables_to_merge)) == 1\n",
        "\n",
        "    for coefficients in coefficients_set:\n",
        "        _merge_with_coeffs(\n",
        "            output_variables,\n",
        "            variables_to_merge,\n",
        "            coefficients=coefficients,\n",
        "            fishers=fishers,\n",
        "            fisher_floor=fisher_floor,\n",
        "            favor_target_model=favor_target_model,\n",
        "            normalization_constants=norm_constants,\n",
        "        )\n",
        "        yield coefficients, output_model\n",
        "\n",
        "\n",
        "def merging_coefficients_search(\n",
        "    mergeable_models,\n",
        "    coefficients_set: Sequence[Sequence[float]],\n",
        "    dataset: tf.data.Dataset,\n",
        "    metric: hfds.Metric,\n",
        "    fishers=None,\n",
        "    fisher_floor: float = 1e-6,\n",
        "    favor_target_model=True,\n",
        "    normalize_fishers=True,\n",
        "    print_results=True,\n",
        "):\n",
        "    merged_models = generate_merged_for_coeffs_set(\n",
        "        mergeable_models,\n",
        "        coefficients_set,\n",
        "        fishers,\n",
        "        fisher_floor=fisher_floor,\n",
        "        favor_target_model=favor_target_model,\n",
        "        normalize_fishers=normalize_fishers,\n",
        "    )\n",
        "    results = []\n",
        "    for coeffs, merged_model in merged_models:\n",
        "        score = evaluation.evaluate_model(merged_model, dataset, metric)\n",
        "        result = MergeResult(coefficients=coeffs, score=score)\n",
        "        results.append(result)\n",
        "        if print_results:\n",
        "            print_merge_result(result)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO-EC6gSSGLn"
      },
      "source": [
        "### main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFtOJgsfR0oo"
      },
      "outputs": [],
      "source": [
        "def load_models():\n",
        "    models = []\n",
        "    for i, model_str in enumerate(FLAGS.models):\n",
        "        model_str = os.path.expanduser(model_str)\n",
        "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "            model_str, from_pt=FLAGS.from_pt\n",
        "        )\n",
        "        models.append(model)\n",
        "        if i == 0:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
        "    return models, tokenizer\n",
        "\n",
        "\n",
        "def load_fishers():\n",
        "    if not FLAGS.fishers:\n",
        "        return None\n",
        "    fishers = []\n",
        "    for fisher_str in FLAGS.fishers:\n",
        "        fisher_str = os.path.expanduser(fisher_str)\n",
        "        fisher = hdf5_util.load_variables_from_hdf5(fisher_str, trainable=False)\n",
        "        fishers.append(fisher)\n",
        "    return fishers\n",
        "\n",
        "\n",
        "def get_coeffs_set():\n",
        "    n_models = len(FLAGS.models)\n",
        "    if FLAGS.coeff_mode == \"grid\":\n",
        "        assert n_models == 2\n",
        "        return merging.create_pairwise_grid_coeffs(FLAGS.n_coeffs)\n",
        "    elif FLAGS.coeff_mode == \"random\":\n",
        "        return merging.create_random_coeffs(n_models, FLAGS.n_coeffs)\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "\n",
        "def get_best_results(results):\n",
        "    return max(results, key=lambda r: evaluation.average_score(r.score))\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    if FLAGS.fishers:\n",
        "        assert len(FLAGS.fishers) == len(FLAGS.models)\n",
        "\n",
        "    models, tokenizer = load_models()\n",
        "\n",
        "    fishers = load_fishers()\n",
        "\n",
        "    ds = data.load_glue_dataset(\n",
        "        task=FLAGS.glue_task,\n",
        "        split=FLAGS.split,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=FLAGS.sequence_length,\n",
        "    )\n",
        "    ds = ds.take(FLAGS.n_examples).batch(FLAGS.batch_size)\n",
        "\n",
        "    metric = evaluation.load_metric_for_glue_task(FLAGS.glue_task)\n",
        "\n",
        "    coefficients_set = get_coeffs_set()\n",
        "\n",
        "    results = merging.merging_coefficients_search(\n",
        "        models,\n",
        "        coefficients_set=coefficients_set,\n",
        "        dataset=ds,\n",
        "        metric=metric,\n",
        "        fishers=fishers,\n",
        "        fisher_floor=FLAGS.fisher_floor,\n",
        "        favor_target_model=FLAGS.favor_target_model,\n",
        "        normalize_fishers=FLAGS.normalize_fishers,\n",
        "    )\n",
        "\n",
        "    best = get_best_results(results)\n",
        "    print(80 * \"*\")\n",
        "    print(\" Best Merge\")\n",
        "    print(80 * \"*\")\n",
        "    merging.print_merge_result(best)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
