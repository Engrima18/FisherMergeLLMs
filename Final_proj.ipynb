{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqMwvfdIGgHe",
        "outputId": "c265fc23-ff36-417f-ffdb-ee282a692b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content\n",
            "Downloading news-headlines-dataset-for-sarcasm-detection.zip to /content\n",
            "  0% 0.00/3.30M [00:00<?, ?B/s]\n",
            "100% 3.30M/3.30M [00:00<00:00, 38.4MB/s]\n",
            "mkdir: cannot create directory ‘dataset’: File exists\n"
          ]
        }
      ],
      "source": [
        "dataset_author = \"rmisra\"\n",
        "dataset_name = \"news-headlines-dataset-for-sarcasm-detection\"\n",
        "dataset_path = dataset_author + \"/\" + dataset_name\n",
        "\n",
        "# Mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "kaggle_creds_path = \"/content/drive/MyDrive/my_kaggle/kaggle.json\"\n",
        "\n",
        "# remove kaggle directory if already exists, then crate it\n",
        "!pwd\n",
        "!rm -r ~/.kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp {kaggle_creds_path} ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! pip install kaggle --quiet\n",
        "! kaggle datasets download -d {dataset_path}\n",
        "\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(f'{dataset_name}.zip', 'r')\n",
        "\n",
        "! mkdir dataset\n",
        "zip_ref.extractall('/content/dataset')\n",
        "zip_ref.close()\n",
        "! rm {dataset_name}.zip\n",
        "\n",
        "# Unmount your Google Drive\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "kBkXeBCZHrk0",
        "outputId": "274900db-d858-4028-cc22-3672694a36da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28614</th>\n",
              "      <td>1</td>\n",
              "      <td>jews to celebrate rosh hashasha or something</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28615</th>\n",
              "      <td>1</td>\n",
              "      <td>internal affairs investigator disappointed con...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28616</th>\n",
              "      <td>0</td>\n",
              "      <td>the most beautiful acceptance speech this week...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28617</th>\n",
              "      <td>1</td>\n",
              "      <td>mars probe destroyed by orbiting spielberg-gat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28618</th>\n",
              "      <td>1</td>\n",
              "      <td>dad clarifies this not a food stop</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28619 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       is_sarcastic                                           headline\n",
              "0                 1  thirtysomething scientists unveil doomsday clo...\n",
              "1                 0  dem rep. totally nails why congress is falling...\n",
              "2                 0  eat your veggies: 9 deliciously different recipes\n",
              "3                 1  inclement weather prevents liar from getting t...\n",
              "4                 1  mother comes pretty close to using word 'strea...\n",
              "...             ...                                                ...\n",
              "28614             1       jews to celebrate rosh hashasha or something\n",
              "28615             1  internal affairs investigator disappointed con...\n",
              "28616             0  the most beautiful acceptance speech this week...\n",
              "28617             1  mars probe destroyed by orbiting spielberg-gat...\n",
              "28618             1                 dad clarifies this not a food stop\n",
              "\n",
              "[28619 rows x 2 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "try:\n",
        "    df = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\", lines = True)\n",
        "except:\n",
        "    df = pd.read_json(\"/content/dataset/Sarcasm_Headlines_Dataset_v2.json\", lines = True)\n",
        "df.head()\n",
        "df.drop(columns=\"article_link\", axis=1, inplace=True)\n",
        "df.head(5)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t5A1WQawVnMj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QjD_rGI-e8b5"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_encode(texts, tokenizer):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "def df_to_dataset(dataframe, tokenizer, shuffle=True, batch_size=32):\n",
        "    dataframe = dataframe.copy()\n",
        "    labels = dataframe.pop('is_sarcastic')\n",
        "    encoded_texts = tokenize_and_encode(dataframe['headline'].tolist(), tokenizer)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((encoded_texts['input_ids'], labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YZpksQHB09Pf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No TPU available!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    BATCH_SIZE = 32 * tpu_strategy.num_replicas_in_sync\n",
        "except:\n",
        "    print(\"No TPU available!\")\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CBmXVzCvBnnZ"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "504d827198704730a68907acaf640af7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\engri\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3dddcb604794d0992071cbaa818006a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3213115d62284e119f200e1f24cf4d31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87762a87ce7842efb411d8e3831775ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5f82666027f4f18895d7e322411afd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "train_data = df_to_dataset(train_df, tokenizer, shuffle=True, batch_size=BATCH_SIZE)\n",
        "test_data = df_to_dataset(test_df, tokenizer, shuffle=False, batch_size=BATCH_SIZE)\n",
        "train_data = train_data.prefetch(buffer_size=AUTOTUNE)\n",
        "test_data = test_data.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z-YStsUTM1q"
      },
      "source": [
        "# Import fine-tuned RoBERTa versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gKa-3RUHTSKr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "mnli_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "            \"textattack/roberta-base-MNLI\", from_pt=True\n",
        "        )\n",
        "\n",
        "sst2_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "            \"textattack/roberta-base-SST-2\", from_pt=True\n",
        "        )\n",
        "\n",
        "### add RoBERTa for sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLzm1-6f3LXH"
      },
      "source": [
        "# Fine-tune Base RoBERTa\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KjP3Zy9O1ZUr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No TPU available!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7966cb4e264842afb7cd1202d4688fbe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    with tpu_strategy.scope():\n",
        "        model_name = \"roberta-base\"\n",
        "        model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True, num_labels=2)\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.),\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=[tf.metrics.SparseCategoricalAccuracy()],\n",
        "            )\n",
        "except:\n",
        "    print(\"No TPU available!\")\n",
        "    model_name = \"roberta-base\"\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True, num_labels=2)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[tf.metrics.SparseCategoricalAccuracy()],\n",
        "        )\n",
        "\n",
        "try:\n",
        "    model.load_weights(f'.\\\\models\\\\{model_name.replace(\"-\",\"_\")}_ft.h5')\n",
        "except:\n",
        "    history=model.fit(train_data, validation_data=test_data, epochs=6, verbose=1)\n",
        "    model.save_weights(f'.\\\\models\\\\{model_name.replace(\"-\",\"_\")}_ft.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DwPv3MYECYM",
        "outputId": "ff998296-05e7-433f-9ea5-7c04c1d241b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_roberta_for_sequence_classification_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
            "_________________________________________________________________\n",
            "classifier (TFRobertaClassif multiple                  592130    \n",
            "=================================================================\n",
            "Total params: 124,647,170\n",
            "Trainable params: 124,647,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.load_weights(f'.\\\\models\\\\{model_name.replace(\"-\",\"_\")}_ft.h5')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GCmvEOl2g00"
      },
      "source": [
        "# Take the weights from my models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "83AQ3yRF-QA6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def fetch_mergeable_vars(mod, to_np=True):\n",
        "    \"\"\"Fetches trainable variables of a given model and converts them to NumPy arrays.\n",
        "\n",
        "    This function supports both models running under a TensorFlow distribution strategy (e.g., TPUs)\n",
        "    and standard models not using distribution strategies (e.g., models downloaded from Hugging Face).\n",
        "\n",
        "    Args:\n",
        "        model: TensorFlow or Hugging Face model whose trainable variables are to be fetched.\n",
        "        strategy: (Optional) The TensorFlow distribution strategy under which the model is running.\n",
        "                  If None, the model is assumed not to be under a distribution strategy.\n",
        "\n",
        "    Returns:\n",
        "        A list of NumPy arrays corresponding to the model's trainable variables.\n",
        "    \"\"\"\n",
        "    def fetch_variables():\n",
        "        body, *head = mod.layers\n",
        "        return [v.numpy() for v in body.trainable_variables]\n",
        "\n",
        "    def distributed_fetch():\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "        body, *head = mod.layers\n",
        "        return strategy.run(lambda: body.trainable_variables)\n",
        "\n",
        "    if to_np:\n",
        "\n",
        "        try:\n",
        "            per_replica_variables = distributed_fetch()\n",
        "            numpy_variables = []\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "            for v in strategy.experimental_local_results(per_replica_variables):\n",
        "                numpy_variables.extend([var.numpy() for var in v])\n",
        "        except:\n",
        "            numpy_variables = fetch_variables()\n",
        "\n",
        "        vars = np.array(numpy_variables, dtype=object)\n",
        "\n",
        "    else:\n",
        "        body, *head = mod.layers\n",
        "        vars = body.trainable_variables\n",
        "\n",
        "    return vars\n",
        "\n",
        "def clone_model(model):\n",
        "    cloned = model.__class__(model.config)\n",
        "    cloned(model.dummy_inputs)\n",
        "    cloned.set_weights(model.get_weights())\n",
        "    return cloned\n",
        "\n",
        "\n",
        "# def assign_params(models_list):\n",
        "#     \"\"\"Assigns values from a list of NumPy arrays to the model's trainable variables.\n",
        "\n",
        "#     This function supports both models running under a TensorFlow distribution strategy (e.g., TPUs)\n",
        "#     and standard models not using distribution strategies (e.g., models downloaded from Hugging Face).\n",
        "\n",
        "#     Args:\n",
        "#         model: TensorFlow or Hugging Face model whose trainable variables are to be updated.\n",
        "#         numpy_variables: A list of NumPy arrays containing the new values for the model's trainable variables.\n",
        "#         strategy: (Optional) The TensorFlow distribution strategy under which the model is running.\n",
        "#                   If None, the model is assumed not to be under a distribution strategy.\n",
        "#     \"\"\"\n",
        "#     # just clone the first model in the list to initialize the parameters\n",
        "#     new_model = clone_model(models_list[0])\n",
        "\n",
        "#     vars_arr = np.array([fetch_mergeable_vars(mod) for mod in models_list], dtype=object)\n",
        "#     # then we have to multiply each matrix in this vars_arr by a corresponding fisher matrix (different for each matrix for each model)\n",
        "#     # the first goal: create a fishers_arr of the same dim of vars_arr (3, 197) with\n",
        "\n",
        "#     def assign_variables():\n",
        "#         body, *head = new_model.layers\n",
        "#         for var, new_val in zip(body.trainable_variables, numpy_variables):\n",
        "#             var.assign(new_val)\n",
        "\n",
        "#     @tf.function\n",
        "#     def distributed_assign():\n",
        "#         strategy = tf.distribute.get_strategy()\n",
        "#         body, *head = model.layers\n",
        "#         for var, new_val in zip(body.trainable_variables, numpy_variables):\n",
        "#             strategy.run(lambda: var.assign(new_val))\n",
        "\n",
        "#     try:\n",
        "#         distributed_assign()\n",
        "#     except:\n",
        "#         assign_variables()\n",
        "\n",
        "\n",
        "\n",
        "# numpy_variables = fetch_trainable_vars(model)\n",
        "numpy_variables_distributed = fetch_mergeable_vars(model)\n",
        "numpy_variables = fetch_mergeable_vars(mnli_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "73UiIhC_FYRz"
      },
      "outputs": [],
      "source": [
        "def _fisher_for_batch(batch, model, variables):\n",
        "    num_labels = model.num_labels\n",
        "    batch = tf.expand_dims(batch, axis=0)\n",
        "    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "        tape.watch(variables)\n",
        "        logits = model(batch, training=False).logits\n",
        "\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "        print(batch.shape)\n",
        "        print(probs.shape)\n",
        "        #print(batch.shape)\n",
        "\n",
        "        with tape.stop_recording():\n",
        "            # sq_grads = tf.square(tape.gradient(log_probs, variables))\n",
        "            sq_grads = [tf.square(tape.gradient(log_probs[:,i], variables)) for i in range(num_labels)]\n",
        "        #tmp=[probs[:,i]*sq_grads[i] for i in range(num_labels)]\n",
        "        # tmp = tf.math.multply(probs, sr_grad)\n",
        "        # logits = tf.squeeze(logits, axis=0)\n",
        "        # log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        # probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "        # grads = tape.gradient(log_probs, variables)\n",
        "        # sq_grads = []\n",
        "        # for i in range(num_labels):\n",
        "        #     log_prob = log_probs[:, i]\n",
        "        #     grad = tape.gradient(log_prob, variables)\n",
        "        #     sq_grad = [probs[:, i] * tf.square(g) for g in grad]\n",
        "        #     sq_grads.append(sq_grad)\n",
        "\n",
        "        # example_fisher = [tf.reduce_sum(g, axis=0) for g in zip(*sq_grads)]\n",
        "\n",
        "    return probs, log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dPHnRPOpiBRY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7-shjII7H4cP"
      },
      "outputs": [],
      "source": [
        "def preprocess(input, label):\n",
        "    return input\n",
        "\n",
        "tr = train_data.map(preprocess)\n",
        "mergeable_vars = fetch_mergeable_vars(model, to_np=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Variable 'tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/self/query/kernel:0' shape=(768, 768) dtype=float32, numpy=\n",
              "array([[ 0.0723473 , -0.05138723,  0.08780038, ..., -0.18677771,\n",
              "        -0.2538248 , -0.05162003],\n",
              "       [-0.00268799,  0.2061689 ,  0.06961331, ...,  0.01774677,\n",
              "         0.04328064, -0.08617006],\n",
              "       [-0.09013141,  0.07399632, -0.05160036, ..., -0.03148533,\n",
              "         0.06424539,  0.10267198],\n",
              "       ...,\n",
              "       [ 0.10370311,  0.06530357, -0.04302413, ..., -0.05038806,\n",
              "         0.07030558, -0.18979007],\n",
              "       [ 0.09016812,  0.06345156, -0.00833152, ...,  0.10257896,\n",
              "        -0.10443079,  0.00326744],\n",
              "       [-0.10294539,  0.12811744,  0.10991048, ..., -0.11664649,\n",
              "         0.01233165, -0.05385955]], dtype=float32)>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mergeable_vars[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 195)\n"
          ]
        }
      ],
      "source": [
        "for example in tr:\n",
        "   print(example.shape)\n",
        "   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(195,)\n"
          ]
        }
      ],
      "source": [
        "for example in tr.unbatch():\n",
        "   print(example.shape)\n",
        "   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "L138VWpFHtcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 195)\n",
            "(1, 2)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[74], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m tr\u001b[38;5;241m.\u001b[39munbatch():\n\u001b[1;32m----> 2\u001b[0m    probs, sq \u001b[38;5;241m=\u001b[39m \u001b[43m_fisher_for_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmergeable_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m    \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[61], line 17\u001b[0m, in \u001b[0;36m_fisher_for_batch\u001b[1;34m(batch, model, variables)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#print(batch.shape)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tape\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;66;03m# sq_grads = tf.square(tape.gradient(log_probs, variables))\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m         sq_grads \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39msquare(tape\u001b[38;5;241m.\u001b[39mgradient(log_probs[:,i], variables)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_labels)]\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#tmp=[probs[:,i]*sq_grads[i] for i in range(num_labels)]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# tmp = tf.math.multply(probs, sr_grad)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# logits = tf.squeeze(logits, axis=0)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# example_fisher = [tf.reduce_sum(g, axis=0) for g in zip(*sq_grads)]\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probs, log_probs\n",
            "Cell \u001b[1;32mIn[61], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#print(batch.shape)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tape\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;66;03m# sq_grads = tf.square(tape.gradient(log_probs, variables))\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m         sq_grads \u001b[38;5;241m=\u001b[39m [\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_labels)]\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#tmp=[probs[:,i]*sq_grads[i] for i in range(num_labels)]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# tmp = tf.math.multply(probs, sr_grad)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# logits = tf.squeeze(logits, axis=0)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# example_fisher = [tf.reduce_sum(g, axis=0) for g in zip(*sq_grads)]\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probs, log_probs\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:10424\u001b[0m, in \u001b[0;36msquare\u001b[1;34m(x, name)\u001b[0m\n\u001b[0;32m  10422\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m  10423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 10424\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msquare_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10425\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10426\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m  10427\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:10459\u001b[0m, in \u001b[0;36msquare_eager_fallback\u001b[1;34m(x, name, ctx)\u001b[0m\n\u001b[0;32m  10458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquare_eager_fallback\u001b[39m(x, name, ctx):\n\u001b[1;32m> 10459\u001b[0m   _attr_T, (x,) \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex128\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10460\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [x]\n\u001b[0;32m  10461\u001b[0m   _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:265\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[1;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[1;32m--> 265\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[0;32m    267\u001b[0m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[0;32m    268\u001b[0m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1566\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1561\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1563\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[0;32m   1565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1566\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1569\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:346\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    344\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    345\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m--> 346\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:283\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    282\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 283\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[0;32m    286\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:308\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    307\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    105\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
          ]
        }
      ],
      "source": [
        "for example in tr.unbatch():\n",
        "   probs, sq = _fisher_for_batch(example, model, mergeable_vars)\n",
        "   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[75], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m probs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tape\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# sq_grads = tf.square(tape.gradient(log_probs, variables))\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     sq_grads \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39msquare(tape\u001b[38;5;241m.\u001b[39mgradient(log_probs[:,i], mergeable_vars)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_labels)]\n",
            "Cell \u001b[1;32mIn[75], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m probs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tape\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# sq_grads = tf.square(tape.gradient(log_probs, variables))\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     sq_grads \u001b[38;5;241m=\u001b[39m [\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmergeable_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_labels)]\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:10424\u001b[0m, in \u001b[0;36msquare\u001b[1;34m(x, name)\u001b[0m\n\u001b[0;32m  10422\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m  10423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 10424\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msquare_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10425\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10426\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m  10427\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:10459\u001b[0m, in \u001b[0;36msquare_eager_fallback\u001b[1;34m(x, name, ctx)\u001b[0m\n\u001b[0;32m  10458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquare_eager_fallback\u001b[39m(x, name, ctx):\n\u001b[1;32m> 10459\u001b[0m   _attr_T, (x,) \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex128\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10460\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [x]\n\u001b[0;32m  10461\u001b[0m   _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:265\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[1;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[1;32m--> 265\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[0;32m    267\u001b[0m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[0;32m    268\u001b[0m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1566\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1561\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1563\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[0;32m   1565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1566\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1569\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:346\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    344\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    345\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m--> 346\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:283\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    282\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 283\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[0;32m    286\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:308\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    307\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
            "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\envs\\deepl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    105\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
          ]
        }
      ],
      "source": [
        "num_labels = model.num_labels\n",
        "example = tf.expand_dims(example, axis=0)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    tape.watch(mergeable_vars)\n",
        "    logits = model(example, training=False).logits\n",
        "\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    with tape.stop_recording():\n",
        "        # sq_grads = tf.square(tape.gradient(log_probs, variables))\n",
        "        sq_grads = [tf.square(tape.gradient(log_probs[:,i], mergeable_vars)) for i in range(num_labels)]\n",
        "    #tmp=[probs[:,i]*sq_grads[i] for i in range(num_labels)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tape.gradient(log_probs[:,1], mergeable_vars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYWO32L5-mvi"
      },
      "outputs": [],
      "source": [
        "sq2 = np.array(sq, dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBM5CqL8YVQt"
      },
      "outputs": [],
      "source": [
        "probs2 = probs.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4Ej8jnLc1cs",
        "outputId": "ed5b22cf-b2f0-4649-c9a0-acd54fa6ce51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(256, 2)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probs2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-HwCA3OZTT_",
        "outputId": "36e7c967-28f9-451d-c936-fc6fbe584f88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(197,)"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sq2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "C5P3dfl5YzLJ",
        "outputId": "8aaf6be4-26f5-48de-b3df-8c349d19f3b3"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (256,2) (197,) ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-56b49dfd644e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msq2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (256,2) (197,) "
          ]
        }
      ],
      "source": [
        "np.multiply(probs2, sq2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1sh5grkEacm",
        "outputId": "d06ecc1b-74fa-4a01-e613-71ebfc8d0325"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "197"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(numpy_variables_distributed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiOupcT26iXE"
      },
      "outputs": [],
      "source": [
        "prova = np.array(numpy_variables, dtype=object)\n",
        "prova2 = np.array(numpy_variables_distributed, dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApYC44-NNrX4"
      },
      "outputs": [],
      "source": [
        "models_list = [model, mnli_model, sst2_model]\n",
        "vars_list = np.array([fetch_trainable_vars(mod) for mod in models_list], dtype=object)\n",
        "pp=np.sum(vars_list, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZkIlgEPOfBm",
        "outputId": "33b77312-63e6-4b37-b7e5-b8b84a21868e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 197)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vars_list.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeN8UmqXPxsj",
        "outputId": "39656d66-80f7-470b-8cc4-b42c9d22f637"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(197,)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pp=np.sum(vars_list, axis=0)\n",
        "pp.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw51sJ5sRxJt"
      },
      "source": [
        "# Fisher Averaging implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYxhqmmg3DfM"
      },
      "source": [
        "### hf_util.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRbSC9Rf3CL4"
      },
      "outputs": [],
      "source": [
        "def get_body_and_head(\n",
        "    model: Union[TFBertPreTrainedModel, TFRobertaPreTrainedModel]\n",
        ") -> Tuple[tf.keras.layers.Layer, tf.keras.layers.Layer]:\n",
        "    body, *head = model.layers\n",
        "    if not head:\n",
        "        head = None\n",
        "    elif len(head) > 1:\n",
        "        raise ValueError(\n",
        "            f\"Expected model to have a single 'head' layer. Instead found {len(head)}. TODO: Support this.\"\n",
        "        )\n",
        "    else:\n",
        "        head = head[0]\n",
        "    return body, head\n",
        "\n",
        "\n",
        "def get_body(model):\n",
        "    return get_body_and_head(model)[0]\n",
        "\n",
        "\n",
        "def get_mergeable_variables(model):\n",
        "    return get_body_and_head(model)[0].trainable_variables\n",
        "\n",
        "\n",
        "def clone_model(model):\n",
        "    cloned = model.__class__(model.config)\n",
        "    cloned(model.dummy_inputs)\n",
        "    cloned.set_weights(model.get_weights())\n",
        "    return cloned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-AJ7yxXWHVf"
      },
      "source": [
        "### evaluation.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVz0oVmpWKms"
      },
      "outputs": [],
      "source": [
        "# import datasets as hfds\n",
        "\n",
        "# def load_metric_for_glue_task(task: str):\n",
        "#     return hfds.load_metric(\"glue\", task)\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataset: tf.data.Dataset, metric: hfds.Metric):\n",
        "    for model_input, gold_references in dataset:\n",
        "        model_predictions = model(model_input).logits\n",
        "        model_predictions = tf.argmax(model_predictions, axis=-1)\n",
        "        metric.add_batch(predictions=model_predictions, references=gold_references)\n",
        "    return metric.compute()\n",
        "\n",
        "\n",
        "def average_score(score):\n",
        "    return sum(score.values()) / len(score.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcVz0L_kVxvu"
      },
      "source": [
        "### fisher.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUpscA3yV30a"
      },
      "outputs": [],
      "source": [
        "def _batch_size(batch):\n",
        "    return tf.shape(batch[\"input_ids\"])[0]\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def _compute_exact_fisher_for_batch(batch, model, variables, expectation_wrt_logits):\n",
        "    assert expectation_wrt_logits, \"TODO: Handle sampling from logits.\"\n",
        "    num_labels = model.num_labels\n",
        "\n",
        "    @tf.function\n",
        "    def fisher_single_example(single_example_batch):\n",
        "        \"\"\"\n",
        "        NOTE: I wrote this with Hugging Face classifiers in mind. There is\n",
        "        probably a good way to do the same thing but with more customizability\n",
        "        to support alternate forms of models.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "            tape.watch(variables)\n",
        "\n",
        "            logits = model(single_example_batch, training=False).logits\n",
        "            # The batch dimension must be 1 to call the model, so we remove it\n",
        "            # here.\n",
        "            logits = tf.squeeze(logits, axis=0)\n",
        "\n",
        "            log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "            probs = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "            sq_grads = []\n",
        "            for i in range(num_labels):\n",
        "                log_prob = log_probs[i]\n",
        "                with tape.stop_recording():\n",
        "                    grad = tape.gradient(log_prob, variables)\n",
        "                    sq_grad = [probs[i] * tf.square(g) for g in grad]\n",
        "                    sq_grads.append(sq_grad)\n",
        "            # Take the average across logits. The per-logit weight was added\n",
        "            # earlier as each per-logit square gradient was weighted by the\n",
        "            # probability of the class according to the output distribution.\n",
        "            example_fisher = [tf.reduce_sum(g, axis=0) for g in zip(*sq_grads)]\n",
        "\n",
        "        return example_fisher\n",
        "\n",
        "    batch = {k: tf.expand_dims(v, axis=1) for k, v in batch.items()}\n",
        "\n",
        "    fishers = tf.vectorized_map(fisher_single_example, batch)\n",
        "    return [tf.reduce_sum(f, axis=0) for f in fishers]\n",
        "\n",
        "\n",
        "def compute_fisher_for_model(\n",
        "    model, dataset: tf.data.Dataset, expectation_wrt_logits=True\n",
        "):\n",
        "    variables = hf_util.get_mergeable_variables(model)\n",
        "\n",
        "    fishers = [\n",
        "        tf.Variable(tf.zeros(w.shape), trainable=False, name=f\"fisher/{w.name}\")\n",
        "        for w in variables\n",
        "    ]\n",
        "\n",
        "    n_examples = 0\n",
        "    for batch, _ in dataset:\n",
        "        n_examples += _batch_size(batch)\n",
        "        batch_fishers = _compute_exact_fisher_for_batch(\n",
        "            batch, model, variables, expectation_wrt_logits=expectation_wrt_logits\n",
        "        )\n",
        "        for f, bf in zip(fishers, batch_fishers):\n",
        "            f.assign_add(bf)\n",
        "\n",
        "    for fisher in fishers:\n",
        "        fisher.assign(fisher / float(n_examples))\n",
        "\n",
        "    return fishers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAgTaE_fWcrZ"
      },
      "source": [
        "### merging.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNaFK8KrWe7F"
      },
      "outputs": [],
      "source": [
        "\"\"\"The code for actually performing the merge.\"\"\"\n",
        "import collections\n",
        "from typing import Optional, Sequence\n",
        "import datasets as hfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from model_merging import hf_util\n",
        "from model_merging import evaluation\n",
        "\n",
        "MergeResult = collections.namedtuple(\"MergeResult\", [\"coefficients\", \"score\"])\n",
        "\n",
        "\n",
        "def print_merge_result(result: MergeResult):\n",
        "    print(f\"Merging coefficients: {result.coefficients}\")\n",
        "    print(\"Scores:\")\n",
        "    for name, value in result.score.items():\n",
        "        print(f\"  {name}: {value}\")\n",
        "\n",
        "\n",
        "def create_pairwise_grid_coeffs(n_weightings: int):\n",
        "    n_weightings -= 2\n",
        "    denom = n_weightings + 1\n",
        "    weightings = [((i + 1) / denom, 1 - (i + 1) / denom) for i in range(n_weightings)]\n",
        "    weightings = [(0.0, 1.0)] + weightings + [(1.0, 0.0)]\n",
        "    weightings.reverse()\n",
        "    return weightings\n",
        "\n",
        "\n",
        "def create_random_coeffs(n_models: int, n_weightings: int, seed: Optional[int] = None):\n",
        "    if seed is not None:\n",
        "        tf.random.set_seed(seed)\n",
        "    dist = tfp.distributions.Dirichlet(tf.ones([n_models]))\n",
        "    return dist.sample(n_weightings, seed=seed).numpy().tolist()\n",
        "\n",
        "\n",
        "def _merge_with_coeffs(\n",
        "    output_variables: Sequence[tf.Variable],\n",
        "    variables_to_merge: Sequence[Sequence[tf.Variable]],\n",
        "    coefficients: Sequence[float],\n",
        "    fishers=None,\n",
        "    fisher_floor: float = 1e-6,\n",
        "    favor_target_model=True,\n",
        "    normalization_constants=None,\n",
        "):\n",
        "    n_models = len(variables_to_merge)\n",
        "    assert len(coefficients) == n_models\n",
        "\n",
        "    if fishers is None:\n",
        "        fishers = n_models * [1.0]\n",
        "    else:\n",
        "        assert len(fishers) == n_models\n",
        "\n",
        "    if normalization_constants is not None:\n",
        "        assert len(normalization_constants) == n_models\n",
        "        coefficients = [w / n for w, n in zip(coefficients, normalization_constants)]\n",
        "\n",
        "    for i, var in enumerate(output_variables):\n",
        "        lhs, rhs = [], []\n",
        "        for j, (mvars, coeff, fisher) in enumerate(\n",
        "            zip(variables_to_merge, coefficients, fishers)\n",
        "        ):\n",
        "            diag = fisher if isinstance(fisher, float) else fisher[i]\n",
        "            if not favor_target_model or j == 0:\n",
        "                diag = tf.maximum(diag, fisher_floor)\n",
        "            mvar = mvars[i]\n",
        "            tmp = coeff * diag\n",
        "            lhs.append(tmp)\n",
        "            rhs.append(tmp * mvar)\n",
        "        rhs = tf.reduce_sum(rhs, axis=0)\n",
        "        lhs = tf.reduce_sum(lhs, axis=0)\n",
        "        var.assign(rhs / lhs)\n",
        "\n",
        "\n",
        "def _l2_norm_of_fisher(fisher):\n",
        "    norm_const = tf.reduce_sum([tf.reduce_sum(tf.square(d)) for d in fisher])\n",
        "    return tf.sqrt(norm_const)\n",
        "\n",
        "\n",
        "def generate_merged_for_coeffs_set(\n",
        "    mergeable_models,\n",
        "    coefficients_set: Sequence[Sequence[float]],\n",
        "    fishers=None,\n",
        "    fisher_floor: float = 1e-6,\n",
        "    favor_target_model=True,\n",
        "    normalize_fishers=True,\n",
        "):\n",
        "    # Create the model to yield, then handle the norm_constants\n",
        "    if normalize_fishers and fishers is not None:\n",
        "        norm_constants = [_l2_norm_of_fisher(f) for f in fishers]\n",
        "    else:\n",
        "        norm_constants = None\n",
        "\n",
        "    # The first model in the list of mergeable models is the \"target\" model and\n",
        "    # the rest are \"donor\" models.\n",
        "    output_model = hf_util.clone_model(mergeable_models[0])\n",
        "    output_variables = hf_util.get_mergeable_variables(output_model)\n",
        "\n",
        "    variables_to_merge = [hf_util.get_mergeable_variables(m) for m in mergeable_models]\n",
        "\n",
        "    # Make sure that all of the variable lists contain exactly the same number\n",
        "    # of variables.\n",
        "    assert len({len(output_variables)} | set(len(v) for v in variables_to_merge)) == 1\n",
        "\n",
        "    for coefficients in coefficients_set:\n",
        "        _merge_with_coeffs(\n",
        "            output_variables,\n",
        "            variables_to_merge,\n",
        "            coefficients=coefficients,\n",
        "            fishers=fishers,\n",
        "            fisher_floor=fisher_floor,\n",
        "            favor_target_model=favor_target_model,\n",
        "            normalization_constants=norm_constants,\n",
        "        )\n",
        "        yield coefficients, output_model\n",
        "\n",
        "\n",
        "def merging_coefficients_search(\n",
        "    mergeable_models,\n",
        "    coefficients_set: Sequence[Sequence[float]],\n",
        "    dataset: tf.data.Dataset,\n",
        "    metric: hfds.Metric,\n",
        "    fishers=None,\n",
        "    fisher_floor: float = 1e-6,\n",
        "    favor_target_model=True,\n",
        "    normalize_fishers=True,\n",
        "    print_results=True,\n",
        "):\n",
        "    merged_models = generate_merged_for_coeffs_set(\n",
        "        mergeable_models,\n",
        "        coefficients_set,\n",
        "        fishers,\n",
        "        fisher_floor=fisher_floor,\n",
        "        favor_target_model=favor_target_model,\n",
        "        normalize_fishers=normalize_fishers,\n",
        "    )\n",
        "    results = []\n",
        "    for coeffs, merged_model in merged_models:\n",
        "        score = evaluation.evaluate_model(merged_model, dataset, metric)\n",
        "        result = MergeResult(coefficients=coeffs, score=score)\n",
        "        results.append(result)\n",
        "        if print_results:\n",
        "            print_merge_result(result)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO-EC6gSSGLn"
      },
      "source": [
        "### main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFtOJgsfR0oo"
      },
      "outputs": [],
      "source": [
        "def load_models():\n",
        "    models = []\n",
        "    for i, model_str in enumerate(FLAGS.models):\n",
        "        model_str = os.path.expanduser(model_str)\n",
        "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "            model_str, from_pt=FLAGS.from_pt\n",
        "        )\n",
        "        models.append(model)\n",
        "        if i == 0:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
        "    return models, tokenizer\n",
        "\n",
        "\n",
        "def load_fishers():\n",
        "    if not FLAGS.fishers:\n",
        "        return None\n",
        "    fishers = []\n",
        "    for fisher_str in FLAGS.fishers:\n",
        "        fisher_str = os.path.expanduser(fisher_str)\n",
        "        fisher = hdf5_util.load_variables_from_hdf5(fisher_str, trainable=False)\n",
        "        fishers.append(fisher)\n",
        "    return fishers\n",
        "\n",
        "\n",
        "def get_coeffs_set():\n",
        "    n_models = len(FLAGS.models)\n",
        "    if FLAGS.coeff_mode == \"grid\":\n",
        "        assert n_models == 2\n",
        "        return merging.create_pairwise_grid_coeffs(FLAGS.n_coeffs)\n",
        "    elif FLAGS.coeff_mode == \"random\":\n",
        "        return merging.create_random_coeffs(n_models, FLAGS.n_coeffs)\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "\n",
        "def get_best_results(results):\n",
        "    return max(results, key=lambda r: evaluation.average_score(r.score))\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    if FLAGS.fishers:\n",
        "        assert len(FLAGS.fishers) == len(FLAGS.models)\n",
        "\n",
        "    models, tokenizer = load_models()\n",
        "\n",
        "    fishers = load_fishers()\n",
        "\n",
        "    ds = data.load_glue_dataset(\n",
        "        task=FLAGS.glue_task,\n",
        "        split=FLAGS.split,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=FLAGS.sequence_length,\n",
        "    )\n",
        "    ds = ds.take(FLAGS.n_examples).batch(FLAGS.batch_size)\n",
        "\n",
        "    metric = evaluation.load_metric_for_glue_task(FLAGS.glue_task)\n",
        "\n",
        "    coefficients_set = get_coeffs_set()\n",
        "\n",
        "    results = merging.merging_coefficients_search(\n",
        "        models,\n",
        "        coefficients_set=coefficients_set,\n",
        "        dataset=ds,\n",
        "        metric=metric,\n",
        "        fishers=fishers,\n",
        "        fisher_floor=FLAGS.fisher_floor,\n",
        "        favor_target_model=FLAGS.favor_target_model,\n",
        "        normalize_fishers=FLAGS.normalize_fishers,\n",
        "    )\n",
        "\n",
        "    best = get_best_results(results)\n",
        "    print(80 * \"*\")\n",
        "    print(\" Best Merge\")\n",
        "    print(80 * \"*\")\n",
        "    merging.print_merge_result(best)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
